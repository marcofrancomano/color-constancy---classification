{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XAtpDQJtm3I_",
        "outputId": "99897c5e-0767-4f7d-d779-d9d033f9e6a4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "ResNet(\n",
              "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "  (relu): ReLU(inplace=True)\n",
              "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "  (layer1): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer3): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (layer4): Sequential(\n",
              "    (0): BasicBlock(\n",
              "      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (downsample): Sequential(\n",
              "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      )\n",
              "    )\n",
              "    (1): BasicBlock(\n",
              "      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "      (relu): ReLU(inplace=True)\n",
              "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
              "  (fc): Linear(in_features=512, out_features=1000, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 44,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "ZYH-JkH-nDmk"
      },
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)\n",
        "\n",
        "def accuracy(out, labels):\n",
        "  _,pred = torch.max(out, dim=1)\n",
        "  return torch.sum(pred==labels).item()\n",
        "\n",
        "\n",
        "#model.fc = torch.nn.Identity()\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, 1)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhTsTi87nhta",
        "outputId": "d4cbba4c-4bd2-46af-ad7f-56a01bc0a18a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[1.]\n",
            "tensor([1.], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import imageio \n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "\n",
        "\n",
        "preprocess=transforms.Compose([\n",
        "    #transforms.Resize(256),\n",
        "    #transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "\n",
        "\n",
        "    def __init__(self, csv_file, root_dir, transform=preprocess):\n",
        "\n",
        "        self.brix= pd.read_csv(csv_file,sep=\",\")\n",
        "        self.root_dir = root_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.brix)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        l=self.brix.iloc[idx,5].split(\".\")\n",
        "        name=l[0]+\"_wb.tiff\"\n",
        "        img_name = os.path.join(self.root_dir,\n",
        "                                name)\n",
        "\n",
        "\n",
        "        image = imageio.imread(img_name)\n",
        "        measures = self.brix.iloc[idx, 1:4]\n",
        "        measures = np.array([measures])\n",
        "        avg=np.mean(measures)\n",
        "        label=np.float32([0])\n",
        "        if avg>16:\n",
        "            label=np.float32([1])\n",
        "        sample = {'image': image, 'label': label}\n",
        "\n",
        "        if self.transform:\n",
        "            sample['image']=np.float32(sample['image'])\n",
        "            sample['image'] = self.transform(sample['image'])\n",
        "\n",
        "        return sample\n",
        "\n",
        "brix_dataset = MyDataset(csv_file='/datalab/brix_labels_07_ott_2021-Foglio1.csv',\n",
        "                                    root_dir='/datalab')\n",
        "\n",
        "input_image=brix_dataset[2]['image']\n",
        "print(brix_dataset[2]['label'])\n",
        "input_tensor=input_image\n",
        "\n",
        "\n",
        "\n",
        "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)\n",
        "probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "print(probabilities)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 456
        },
        "id": "uz2wpm9l8_VA",
        "outputId": "331f56fe-6a17-451e-e899-a763303ecf2e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:28: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch [1/5], Step [0/50], Loss: -0.0000\n",
            "Epoch [1/5], Step [20/50], Loss: -0.0000\n",
            "Epoch [1/5], Step [40/50], Loss: -0.0000\n",
            "\n",
            "train-loss: 0.0000, train-acc: 100.0000\n",
            "Epoch 2\n",
            "\n",
            "Epoch [2/5], Step [0/50], Loss: -0.0000\n",
            "Epoch [2/5], Step [20/50], Loss: -0.0000\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-53-668109480ff2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     31\u001b[0m      \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m      \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m      \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m      \u001b[0;31m#_,pred = torch.max(outputs, dim=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m      \u001b[0;32mif\u001b[0m \u001b[0mprobabilities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "n_epochs = 5\n",
        "print_every = 10\n",
        "valid_loss_min = np.Inf\n",
        "val_loss = []\n",
        "val_acc = []\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "dataset_size=MyDataset.__len__(brix_dataset)\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "  running_loss = 0.0\n",
        "  correct = 0\n",
        "  total=0\n",
        "  print(f'Epoch {epoch}\\n')\n",
        "  \n",
        "  for batch_idx in range(dataset_size):\n",
        "     sample=brix_dataset[batch_idx]\n",
        "     data_=sample['image']\n",
        "     target_=torch.tensor(sample['label']).unsqueeze(0)\n",
        "     if torch.cuda.is_available():\n",
        "      data_, target_ = data_.to('cuda'), target_.to('cuda')\n",
        "     optimizer.zero_grad()\n",
        "     data_ = data_.unsqueeze(0)      \n",
        "     outputs = model(data_)\n",
        "     probabilities = torch.nn.functional.softmax(outputs)\n",
        "     #print(outputs,\"and\",probabilities)\n",
        "     loss = criterion(probabilities, target_)\n",
        "     loss.backward()\n",
        "     optimizer.step()\n",
        "     running_loss += loss.item()\n",
        "     #_,pred = torch.max(outputs, dim=1)\n",
        "     if probabilities[0]>0.5:\n",
        "       pred=torch.tensor(np.float32([1]))\n",
        "     else:\n",
        "        pred=torch.tensor(np.float32([0]))\n",
        "     pred=pred.to('cuda')\n",
        "     correct += torch.sum(pred==target_).item()\n",
        "     total += target_.size(0)\n",
        "     if (batch_idx) % 20 == 0:\n",
        "      print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
        "        .format(epoch, n_epochs, batch_idx, dataset_size, loss.item()))\n",
        "  train_acc.append(100 * correct / total)\n",
        "  train_loss.append(running_loss/dataset_size)\n",
        "  print(f'\\ntrain-loss: {np.mean(train_loss):.4f}, train-acc: {(100 * correct/total):.4f}')\n",
        "  batch_loss = 0  \n",
        "  total_t=0\n",
        "  correct_t=0\n",
        "  #VALIDATE\n",
        "  #with torch.no_grad():\n",
        "    #model.eval()\n",
        "    #for data_t, target_t in (test_dataloader):\n",
        "      #if torch.cuda.is_available():\n",
        "        #data_t, target_t = data_t.to('cuda0'), target_t.to('cuda0')\n",
        "      #outputs_t = model(data_t)\n",
        "      #loss_t = criterion(outputs_t, target_t)\n",
        "      #batch_loss += loss_t.item()\n",
        "      #_,pred_t = torch.max(outputs_t, dim=1)\n",
        "      #correct_t += torch.sum(pred_t==target_t).item()\n",
        "      #total_t += target_t.size(0)\n",
        "      #val_acc.append(100 * correct_t/total_t)\n",
        "      #val_loss.append(batch_loss/len(test_dataloader))\n",
        "      #network_learned = batch_loss < valid_loss_min\n",
        "      #print(f'validation loss: {np.mean(val_loss):.4f}, validation acc: {(100 * correct_t/total_t):.4f}\\n')     \n",
        "      #if network_learned:\n",
        "        #valid_loss_min = batch_loss\n",
        "        #torch.save(net.state_dict(), 'resnet.pt')\n",
        "        #print('Improvement-Detected, save-model')\n",
        "      #model.train()\n",
        "  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3_XPJ2tW98y",
        "outputId": "3a1008fb-318a-4e47-d085-cc78d4e21cf0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.is_available()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "CanopiesNet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
