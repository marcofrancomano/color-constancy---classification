{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=True)\n",
        "model.eval()\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "metadata": {
        "id": "-bJRxuNJqW1d",
        "outputId": "cfd88c07-a984-47c3-d57c-e044500ccb35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "def accuracy(out, labels):\n",
        "  _,pred = torch.max(out, dim=1)\n",
        "  return torch.sum(pred==labels).item()\n",
        "\n",
        "\n",
        "num_ftrs = model.fc.in_features\n",
        "model.fc = torch.nn.Linear(num_ftrs, 1)\n",
        "t=torch.ones([1], dtype=torch.float64)\n",
        "t[0]=0.4853\n",
        "t= t.to('cuda')\n",
        "unb_criterion = torch.nn.BCEWithLogitsLoss(weight =t)\n",
        "criterion = torch.nn.BCEWithLogitsLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.0001, momentum=0.9)"
      ],
      "metadata": {
        "id": "A62EuZFkqszK"
      },
      "execution_count": 109,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision.transforms.transforms import Normalize\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import imageio \n",
        "import matplotlib.pyplot as plt\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import os\n",
        "import albumentations as A\n",
        "import cv2\n",
        "!pip install albumentations==0.4.6\n",
        "from albumentations.pytorch.transforms import ToTensorV2\n",
        "\n",
        "\n",
        "preprocess_train = A.Compose([\n",
        "    A.Normalize(max_pixel_value=1.0),\n",
        "    A.RandomCropNearBBox(0.15),\n",
        "    A.HorizontalFlip(),\n",
        "    ToTensorV2(),\n",
        "\n",
        "\n",
        "])\n",
        "preprocess_val = A.Compose([\n",
        "    A.Normalize(max_pixel_value=1.0),\n",
        "    A.RandomCropNearBBox(0),\n",
        "    ToTensorV2(),\n",
        "\n",
        "\n",
        "])\n",
        "\n",
        "\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, csv_file, root_dir, transform,train,dataset=\"tiff_wb\"):\n",
        "        split = \"train\" if train else \"val\"\n",
        "        self.brix= pd.read_csv(csv_file,sep=\",\")\n",
        "        self.root_dir = os.path.join(root_dir, dataset, split)\n",
        "        self.transform = transform\n",
        "\n",
        "        self.brix = self.brix[self.brix[\"split\"] == split]\n",
        "        self.brix = self.brix.reset_index()\n",
        "        #self.img_labels = csv[[\"file_id\", \"brix\", \"label\"]]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.brix)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "  \n",
        "        l=self.brix.iloc[idx,2].split(\".\")\n",
        "        name=l[0]+\".tiff\"\n",
        "        img_name = os.path.join(self.root_dir, name)\n",
        "\n",
        "        image = imageio.imread(img_name)\n",
        "        # print(\"img\", np.max(image))\n",
        "        image = np.asarray(image, dtype=np.float32)  # convert imageio to ndarray\n",
        "        image = image / (2**16-1)  # normalize between 0 and 1\n",
        "\n",
        "        # Cropping box\n",
        "        box_path = os.path.join('/content/drive/MyDrive/canopies-utilities', 'bboxes', f'{l[0]}.txt')\n",
        "        bbox = np.loadtxt(box_path, delimiter=\" \", dtype=np.float32)\n",
        "        bbox = bbox[1:]\n",
        "        # Convert to pascal format xmin ymin xmax ymax\n",
        "        x_min = int((bbox[0] - bbox[2] / 2.0) * 1500)\n",
        "        x_max = int((bbox[0] + bbox[2] / 2.0) * 1500)\n",
        "        y_min = int((bbox[1] - bbox[3] / 2.0) * 2000)\n",
        "        y_max = int((bbox[1] + bbox[3] / 2.0) * 2000)\n",
        "        \n",
        "        if self.transform is not None:\n",
        "          image = self.transform(image=image, cropping_bbox=[x_min, y_min, x_max, y_max])[\"image\"]\n",
        "\n",
        "        else:\n",
        "        #image = torch.as_tensor(image['image'])  # convert to tensor\n",
        "          image = torch.as_tensor(image)\n",
        "          image = image.permute((2, 0, 1))  # permute the image from HWC to CHW format\n",
        "        image=transforms.Resize((512,512))(image)\n",
        "        # print(image.dtype)\n",
        "        # print(image.shape)\n",
        "        label=np.float32(self.brix.iloc[idx,4])\n",
        "        sample = {'image': image, 'label': label}\n",
        "        # print(\"img\", np.max(sample['image']))\n",
        "\n",
        "        return sample\n",
        "\n",
        "#brix_dataset = MyDataset(csv_file=open('/content/drive/MyDrive/datasets/brix_labels.csv', 'r'),\n",
        "                                     #root_dir='drive/MyDrive/datasets',transform=None,train=True)\n",
        "train_dataset = MyDataset(csv_file=open('/content/drive/MyDrive/datasets/brix_labels.csv', 'r'),\n",
        "                                     root_dir='drive/MyDrive/datasets',transform=preprocess_train,train=True)\n",
        "val_dataset = MyDataset(csv_file=open('/content/drive/MyDrive/datasets/brix_labels.csv', 'r'),\n",
        "                                     root_dir='drive/MyDrive/datasets',transform=preprocess_val,train=False)\n",
        "                           \n",
        "sample = train_dataset[2]\n",
        "input_image=sample['image']\n",
        "print(sample['label'])\n",
        "input_tensor=input_image\n",
        "input_batch = input_tensor.unsqueeze(0)  # create a mini-batch as expected by the model\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)\n",
        "    print(output)\n",
        "    print(input_batch.shape)\n",
        "probabilities = torch.sigmoid(output[0])\n",
        "print(input_batch)\n",
        "print(\"Prob:\", probabilities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xEb9sYUb5PqF",
        "outputId": "1733006b-8a47-4de8-b3d4-36cb8a07a71e"
      },
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: albumentations==0.4.6 in /usr/local/lib/python3.7/dist-packages (0.4.6)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (4.1.2.30)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (3.13)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.4.1)\n",
            "Requirement already satisfied: imgaug>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (0.4.0)\n",
            "Requirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from albumentations==0.4.6) (1.21.6)\n",
            "Requirement already satisfied: scikit-image>=0.14.2 in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (0.18.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (7.1.2)\n",
            "Requirement already satisfied: imageio in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (2.4.1)\n",
            "Requirement already satisfied: Shapely in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.8.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (3.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from imgaug>=0.4.0->albumentations==0.4.6) (1.15.0)\n",
            "Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2021.11.2)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (1.3.0)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from scikit-image>=0.14.2->imgaug>=0.4.0->albumentations==0.4.6) (2.6.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (0.11.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (3.0.9)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (1.4.3)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (2.8.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from kiwisolver>=1.0.1->matplotlib->imgaug>=0.4.0->albumentations==0.4.6) (4.1.1)\n",
            "1.0\n",
            "tensor([[-0.1233]], device='cuda:0')\n",
            "torch.Size([1, 3, 512, 512])\n",
            "tensor([[[[-0.6186, -0.9823, -1.3343,  ...,  1.9762,  1.6321,  1.3270],\n",
            "          [ 0.7600, -0.7441, -1.3930,  ...,  2.2489,  2.1914,  2.0151],\n",
            "          [-0.8183, -1.1823, -1.1610,  ...,  2.2489,  2.2489,  2.2489],\n",
            "          ...,\n",
            "          [-1.9115, -1.9128, -1.9066,  ..., -0.8254, -0.8816, -1.5127],\n",
            "          [-1.8921, -1.9097, -1.8817,  ..., -0.7627, -1.0823, -1.5046],\n",
            "          [-1.8995, -1.9004, -1.9158,  ..., -1.2906, -1.4645, -1.4885]],\n",
            "\n",
            "         [[-1.1519, -1.1523, -1.4613,  ...,  1.4482,  1.0983,  0.7779],\n",
            "          [-0.1985, -0.8213, -1.4736,  ...,  2.0017,  1.7072,  1.4634],\n",
            "          [-1.0966, -1.4140, -1.4135,  ...,  2.3501,  2.2626,  2.1148],\n",
            "          ...,\n",
            "          [-1.8068, -1.8126, -1.8115,  ..., -1.3388, -1.3713, -1.5742],\n",
            "          [-1.8113, -1.8177, -1.8260,  ..., -1.3391, -1.3642, -1.5949],\n",
            "          [-1.8130, -1.8217, -1.8029,  ..., -1.5212, -1.5701, -1.6657]],\n",
            "\n",
            "         [[-1.2944, -1.2125, -1.3750,  ...,  0.5596,  0.2120,  0.0059],\n",
            "          [-0.4777, -0.7502, -1.3338,  ...,  0.9688,  0.6816,  0.4018],\n",
            "          [-1.0520, -1.2536, -1.3795,  ...,  1.2046,  1.2203,  1.0443],\n",
            "          ...,\n",
            "          [-1.5805, -1.5677, -1.5726,  ..., -1.3179, -1.3402, -1.4288],\n",
            "          [-1.5769, -1.5377, -1.5751,  ..., -1.3600, -1.3258, -1.4567],\n",
            "          [-1.6006, -1.5556, -1.5842,  ..., -1.4274, -1.4669, -1.5223]]]],\n",
            "       device='cuda:0')\n",
            "Prob: tensor([0.4692], device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from sklearn.utils import resample\n",
        "\n",
        "#training_data,test_data=torch.utils.data.random_split(brix_dataset,[120,30])\n",
        "#train_dataloader = DataLoader(training_data, batch_size=6, shuffle=True)\n",
        "#test_dataloader = DataLoader(test_data, batch_size=6, shuffle=True)\n",
        "\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=6, shuffle=True)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=1)"
      ],
      "metadata": {
        "id": "N3leMbdx55xC"
      },
      "execution_count": 111,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn.metrics\n",
        "\n",
        "def evaluate(model,dataloader,criterion,device):\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "    val_loss=0.0\n",
        "    preds_all=[]\n",
        "    labels_all=[]\n",
        "    i=0\n",
        "    for data in dataloader:\n",
        "      data_= data['image']\n",
        "      target_=data['label']\n",
        "      #target_=torch.tensor(target_).unsqueeze(0)\n",
        "      if torch.cuda.is_available():\n",
        "        data_, target_ = data_.to(device), target_.to(device)\n",
        "    \n",
        "      outputs=model(data_)\n",
        "      outputs=outputs.reshape(data_.size(0))\n",
        "      pred=1 if torch.sigmoid(outputs).item()>0.5 else 0\n",
        "      preds_all.append(pred)\n",
        "      labels_all.append(int(target_.item()))\n",
        "      loss=criterion(outputs,target_)\n",
        "      val_loss+=(loss.item()-val_loss/(i+1)) #calcolo la media volta per volta\n",
        "      i+=1\n",
        "    f1_score=sklearn.metrics.f1_score(labels_all,preds_all)\n",
        "    print(f'Validation loss: {val_loss:.4f}')\n",
        "    print(f'F1 score: {f1_score:.4f}')\n",
        "    #print(f'Predictions: {preds_all}')\n",
        "    #print(f'Labels: {labels_all}')\n",
        "  model.train()\n",
        "  return val_loss\n",
        "\n",
        "        \n"
      ],
      "metadata": {
        "id": "VLvb4BrBE2he"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_epochs = 20\n",
        "valid_loss_min = np.Inf\n",
        "val_loss = []\n",
        "val_acc = []\n",
        "train_loss = 0\n",
        "dataset_size=len(train_dataset)\n",
        "preds_all=[]\n",
        "labels_all=[]\n",
        "\n",
        "model.train()\n",
        "\n",
        "for epoch in range(1, n_epochs+1):\n",
        "  running_loss = 0.0\n",
        "  correct = 0\n",
        "  total=0\n",
        "  print(f'Epoch {epoch}\\n')\n",
        "  \n",
        "  for data in train_dataloader:\n",
        "    data_= data['image']\n",
        "    target_=data['label']\n",
        "    #data_ = data_.unsqueeze(0) \n",
        "    #target_=torch.tensor(target_).unsqueeze(0)\n",
        "    if torch.cuda.is_available():\n",
        "      data_, target_ = data_.to('cuda'), target_.to('cuda')\n",
        "    optimizer.zero_grad()\n",
        "    \n",
        "    target_=target_.reshape([6])\n",
        "    #print(data_.shape)     \n",
        "    outputs = model(data_)\n",
        "    #print(outputs,target_)\n",
        "    outputs=outputs.reshape(data_.size(0))\n",
        "    #pred=1 if torch.sigmoid(outputs).item() >0.5 else 0\n",
        "    #preds_all.append(pred)\n",
        "    #labels_all.append(int(target_.item()))\n",
        "    loss = unb_criterion(outputs, target_)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    train_loss += loss.item()\n",
        "  print(f'\\ntrain-loss: {train_loss/dataset_size:.4f}')\n",
        "  #f1_score=sklearn.metrics.f1_score(labels_all,preds_all)\n",
        "  #print(f'F1 score: {f1_score:.4f}')\n",
        "  train_loss = 0\n",
        "  # Evaluate model after each epoch\n",
        "  evaluate(model,val_dataloader,criterion,'cuda')"
      ],
      "metadata": {
        "id": "oYW5-sP6BWkR",
        "outputId": "0e8f6711-00df-44eb-b7c8-80d63b9a35f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1\n",
            "\n",
            "\n",
            "train-loss: 0.0546\n",
            "Validation loss: 3.8662\n",
            "F1 score: 0.6667\n",
            "Epoch 2\n",
            "\n",
            "\n",
            "train-loss: 0.0510\n",
            "Validation loss: 4.1638\n",
            "F1 score: 0.6667\n",
            "Epoch 3\n",
            "\n",
            "\n",
            "train-loss: 0.0505\n",
            "Validation loss: 4.3069\n",
            "F1 score: 0.6667\n",
            "Epoch 4\n",
            "\n",
            "\n",
            "train-loss: 0.0488\n",
            "Validation loss: 4.4076\n",
            "F1 score: 0.6667\n",
            "Epoch 5\n",
            "\n",
            "\n",
            "train-loss: 0.0489\n",
            "Validation loss: 4.3932\n",
            "F1 score: 0.6667\n",
            "Epoch 6\n",
            "\n",
            "\n",
            "train-loss: 0.0492\n",
            "Validation loss: 4.3620\n",
            "F1 score: 0.6667\n",
            "Epoch 7\n",
            "\n",
            "\n",
            "train-loss: 0.0487\n",
            "Validation loss: 4.4657\n",
            "F1 score: 0.6667\n",
            "Epoch 8\n",
            "\n",
            "\n",
            "train-loss: 0.0489\n",
            "Validation loss: 4.5128\n",
            "F1 score: 0.6667\n",
            "Epoch 9\n",
            "\n",
            "\n",
            "train-loss: 0.0490\n",
            "Validation loss: 4.6705\n",
            "F1 score: 0.6667\n",
            "Epoch 10\n",
            "\n",
            "\n",
            "train-loss: 0.0477\n",
            "Validation loss: 4.6575\n",
            "F1 score: 0.6667\n",
            "Epoch 11\n",
            "\n",
            "\n",
            "train-loss: 0.0476\n",
            "Validation loss: 4.7132\n",
            "F1 score: 0.6667\n",
            "Epoch 12\n",
            "\n",
            "\n",
            "train-loss: 0.0472\n",
            "Validation loss: 4.5807\n",
            "F1 score: 0.6667\n",
            "Epoch 13\n",
            "\n",
            "\n",
            "train-loss: 0.0491\n",
            "Validation loss: 4.6327\n",
            "F1 score: 0.6667\n",
            "Epoch 14\n",
            "\n",
            "\n",
            "train-loss: 0.0468\n",
            "Validation loss: 4.7160\n",
            "F1 score: 0.6667\n",
            "Epoch 15\n",
            "\n",
            "\n",
            "train-loss: 0.0484\n",
            "Validation loss: 4.6811\n",
            "F1 score: 0.6667\n",
            "Epoch 16\n",
            "\n",
            "\n",
            "train-loss: 0.0465\n",
            "Validation loss: 4.8276\n",
            "F1 score: 0.6667\n",
            "Epoch 17\n",
            "\n",
            "\n",
            "train-loss: 0.0472\n",
            "Validation loss: 4.7139\n",
            "F1 score: 0.6667\n",
            "Epoch 18\n",
            "\n",
            "\n",
            "train-loss: 0.0466\n",
            "Validation loss: 4.8452\n",
            "F1 score: 0.6667\n",
            "Epoch 19\n",
            "\n",
            "\n",
            "train-loss: 0.0463\n",
            "Validation loss: 4.8072\n",
            "F1 score: 0.6667\n",
            "Epoch 20\n",
            "\n",
            "\n",
            "train-loss: 0.0472\n",
            "Validation loss: 5.0381\n",
            "F1 score: 0.6667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "# since we're not training, we don't need to calculate the gradients for our outputs\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    for data in val_dataloader:\n",
        "        images= data['image']\n",
        "        labels=data['label']\n",
        "        images, labels = images.cuda(), labels.cuda()\n",
        "        #images, labels = images.to('cuda'), labels.to('cuda')\n",
        "        \n",
        "        # calculate outputs by running images through the network\n",
        "        outputs = model(images)\n",
        "        probabilities = torch.sigmoid(outputs)\n",
        "        #print(outputs.shape,probabilities.shape,labels.shape)\n",
        "        # the class with the highest energy is what we choose as prediction\n",
        "        #_, predicted = torch.max(outputs.data, 1)\n",
        "        pred=torch.zeros([probabilities.shape[0]])\n",
        "        print(probabilities)\n",
        "        pred=pred.cuda()\n",
        "        for i in range(probabilities.shape[0]):\n",
        "          if(probabilities[i]>0.5):\n",
        "            pred[i]=1\n",
        "\n",
        "        #print(probabilities,labels)\n",
        "        #print(labels.size(0)))\n",
        "        #print(pred)\n",
        "        total += labels.size(0)\n",
        "        correct += (pred == labels).sum().item()\n",
        "        print(correct)\n",
        "\n",
        "print(f'Accuracy of the network on the 10 test images: {100 * correct // total} %')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY9iBfFyhyzQ",
        "outputId": "6ffc2fa9-6277-4c1d-a2dc-abfb7a1d51f0"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[0.5557]], device='cuda:0')\n",
            "1\n",
            "tensor([[0.5915]], device='cuda:0')\n",
            "1\n",
            "tensor([[0.6439]], device='cuda:0')\n",
            "2\n",
            "tensor([[0.6543]], device='cuda:0')\n",
            "2\n",
            "tensor([[0.6789]], device='cuda:0')\n",
            "3\n",
            "tensor([[0.7525]], device='cuda:0')\n",
            "3\n",
            "tensor([[0.7670]], device='cuda:0')\n",
            "3\n",
            "tensor([[0.8040]], device='cuda:0')\n",
            "4\n",
            "tensor([[0.8638]], device='cuda:0')\n",
            "4\n",
            "tensor([[0.7967]], device='cuda:0')\n",
            "5\n",
            "Accuracy of the network on the 10 test images: 50 %\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ColorCostancyNet-V2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}